{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "class Layer:\n",
    "    def __init__(self, width, prev_width, activation_fun, d_fun):\n",
    "        self.width = width\n",
    "        self.prev_width = prev_width\n",
    "        self.W = self.__initializeWeights(width, prev_width)\n",
    "        self.activation_fun = activation_fun\n",
    "        self.weightedSums = np.zeros(prev_width + 1)\n",
    "        self.activations = np.zeros(prev_width + 1)\n",
    "        self.d_fun = d_fun\n",
    "        \n",
    "    def __initializeWeights(self, width, prev_width):\n",
    "        W = np.random.rand(prev_width + 1, width)\n",
    "        return W\n",
    "\n",
    "    def propogate(self, inputs):\n",
    "        inputs = np.append(1, inputs) # Prepend 1 to calc bias\n",
    "        self.weightedSums = np.matmul(inputs, self.W)\n",
    "        self.activations = [self.activation_fun(x) for x in self.weightedSums]\n",
    "        \n",
    "    def updateWeight(self, tail_neuron, head_neuron, deriv, step):\n",
    "        self.W[tail_neuron][head_neuron] -= deriv * step\n",
    "        \n",
    "    def getWidth(self):\n",
    "        return self.width\n",
    "    \n",
    "    def getWeights(self):\n",
    "        return self.W\n",
    "    \n",
    "    def d_sigma(self, x):\n",
    "        return self.d_fun(x)\n",
    "    \n",
    "    def getWeightedSums(self):\n",
    "        return self.weightedSums\n",
    "    \n",
    "    def getActivations(self):\n",
    "        return self.activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, hiddenLayerWidths, activation_functions, d_functions, train_X, train_y, step):\n",
    "        self.train_X = train_X\n",
    "        self.train_y = train_y\n",
    "        self.input_N = len(train_X[0])\n",
    "        self.activation_funs = activation_functions\n",
    "        self.d_funs = d_functions\n",
    "        self.layers = self.__initializeLayers(hiddenLayerWidths)\n",
    "        self.sampleLayerWeightedSums = [[[] for i in range(len(self.layers))] for x in range(len(self.train_X))]\n",
    "        self.sampleLayerActivations = [[[] for i in range(len(self.layers))] for x in range(len(self.train_X))]\n",
    "        self.step = step\n",
    "        \n",
    "    def __initializeLayers(self, hiddenLayerWidths):\n",
    "        layers = []\n",
    "        prev_width = self.input_N\n",
    "        for width, activation_fun, derivative_fun in zip(hiddenLayerWidths, self.activation_funs, self.d_funs):\n",
    "            layers.append(Layer(width, prev_width, activation_fun, derivative_fun))\n",
    "            prev_width = width\n",
    "        return layers\n",
    "    \n",
    "    def __computeLayers(self):\n",
    "        for sample_n in range(len(self.train_X)):\n",
    "            currInp = self.train_X[sample_n]\n",
    "            for layer_l in range(len(self.layers)):\n",
    "                self.layers[layer_l].propogate(currInp)\n",
    "                self.sampleLayerWeightedSums[sample_n][layer_l] = self.layers[layer_l].getWeightedSums()\n",
    "                activations = self.layers[layer_l].getActivations()\n",
    "                self.sampleLayerActivations[sample_n][layer_l] = activations\n",
    "                currInp = activations\n",
    "    \n",
    "    def getLoss(self): # Loss function: C = 1/2 * sum(||y - f(x)||^2)\n",
    "        self.__computeLayers()\n",
    "        loss = 0\n",
    "        for sample_n in range(len(self.train_y)):\n",
    "            diff = np.subtract(self.train_y[sample_n], self.sampleLayerActivations[sample_n][-1])\n",
    "            loss += np.linalg.norm(diff) **2\n",
    "        return loss / 2\n",
    "\n",
    "    def getGradient(self):\n",
    "        errors = [[0 for i in range(self.layers[x].getWidth())] for x in range(len(self.layers))]\n",
    "        \n",
    "        # Compute last layers error\n",
    "        for sample_n in range(len(self.train_y)):\n",
    "            a_grad = np.subtract(self.sampleLayerActivations[sample_n][-1], self.train_y[sample_n])\n",
    "            d_sigma = [self.layers[-1].d_sigma(neuronSum) \n",
    "                       for neuronSum in self.sampleLayerWeightedSums[sample_n][-1]]\n",
    "            errors[-1] = np.add(errors[-1], np.multiply(a_grad, d_sigma))\n",
    "            \n",
    "        # Compute other layers errors\n",
    "        for layer_l in range(len(self.layers) - 2, -1, -1):\n",
    "            left = np.matmul(self.layers[layer_l + 1].getWeights()[1:], errors[layer_l + 1])\n",
    "            for sample_n in range(len(self.train_y)):\n",
    "                right = [self.layers[layer_l].d_sigma(neuronSum)\n",
    "                         for neuronSum in self.sampleLayerWeightedSums[sample_n][layer_l]]\n",
    "            errors[layer_l] = np.add(errors[layer_l], np.multiply(left, right))\n",
    "        \n",
    "        activationSums = [[0 for i in range(self.layers[x].getWidth())] for x in range(len(self.layers))]\n",
    "        \n",
    "        # Compute sum of all neuron activations over all samples\n",
    "        for sample_n in range(len(self.train_X)):\n",
    "            for layer_l in range(len(self.layers)):\n",
    "                for neuron_n in range(self.layers[layer_l].getWidth()):\n",
    "                    activationSums[layer_l] = np.add(activationSums[layer_l], self.sampleLayerActivations[sample_n][layer_l])\n",
    "        \n",
    "        # Preupdate first layer\n",
    "        firstActivationSum = [0 for x in range(self.input_N)]\n",
    "        for i in range(len(self.train_X)):\n",
    "            firstActivationSum = np.add(self.train_X[i], firstActivationSum)\n",
    "            \n",
    "        for neuron_n in range(self.layers[0].getWidth()):\n",
    "            error = errors[0][neuron_n]\n",
    "            self.layers[0].updateWeight(0, neuron_n, error, self.step)\n",
    "            for inc_n in range(1, self.input_N + 1):\n",
    "                self.layers[0].updateWeight(inc_n, neuron_n, error * firstActivationSum[inc_n - 1], self.step)\n",
    "\n",
    "        # Update rest of layer\n",
    "        for layer_l in range(1, len(self.layers)):\n",
    "            for neuron_n in range(self.layers[layer_l].getWidth()):\n",
    "                error = errors[layer_l][neuron_n]\n",
    "                self.layers[layer_l].updateWeight(0, neuron_n, error, self.step)\n",
    "                for inc_n in range(1, self.layers[layer_l - 1].getWidth() + 1):\n",
    "                    self.layers[layer_l].updateWeight(inc_n, neuron_n, error * activationSums[layer_l - 1][inc_n - 1], self.step)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1391679048.821583\n",
      "1149478489.7229633\n",
      "952612209.0076916\n",
      "791645961.6862352\n",
      "659386883.9527857\n",
      "550271777.7876389\n",
      "459944433.8723171\n",
      "384957315.16377264\n",
      "322556948.8935373\n",
      "270526842.5671163\n",
      "227070683.46050406\n",
      "190724241.5091731\n",
      "160288054.3758148\n",
      "134775384.4086815\n",
      "113371553.92049961\n",
      "95401867.01708782\n",
      "80306088.44316955\n",
      "67617984.70881394\n",
      "56948812.99720853\n",
      "47973917.13834581\n",
      "40421789.46294148\n",
      "34065104.461024344\n",
      "28713339.846958157\n",
      "24206683.281902906\n",
      "20410985.92814266\n",
      "17213572.385623075\n",
      "14519754.098666672\n",
      "12249922.703184392\n",
      "10337122.971237043\n",
      "8725023.445159538\n",
      "7366217.612732071\n",
      "6220800.365550054\n",
      "5255174.116837338\n",
      "4441046.800487836\n",
      "3754590.3911218406\n",
      "3175733.8562057223\n",
      "2687568.7959664455\n",
      "2275849.6188570014\n",
      "1928573.0780656566\n",
      "1635624.4688211952\n",
      "1388479.8463054341\n",
      "1179955.342168353\n",
      "1003996.0928327849\n",
      "855498.493128965\n",
      "730160.4938485451\n",
      "624355.5041265961\n",
      "535026.1660682373\n",
      "459594.8620416531\n",
      "395888.3130908774\n",
      "342074.0454020664\n",
      "296606.85354508244\n",
      "258183.68504601193\n",
      "225705.61970441623\n",
      "198245.82646756808\n",
      "175022.55691040526\n",
      "155376.38272571628\n",
      "138751.00953620428\n",
      "124677.10452014605\n",
      "112758.6639222282\n",
      "102661.52112878345\n",
      "94103.6588319438\n",
      "86847.04174893118\n",
      "80690.73096461015\n",
      "75465.07854363952\n",
      "71026.83272088486\n",
      "67255.0106577297\n",
      "64047.41823293057\n",
      "61317.71528086836\n",
      "58992.94065453085\n",
      "57011.42494430419\n",
      "55321.030021940715\n",
      "53877.66413470246\n",
      "52644.02932819252\n",
      "51588.564764017836\n",
      "50684.55521931077\n",
      "49909.378877020936\n",
      "49243.87258008519\n",
      "48671.796148335416\n",
      "48179.38024449481\n",
      "47754.94470965298\n",
      "47388.57634029784\n",
      "47071.85680851758\n",
      "46797.63288493751\n",
      "46559.822353023184\n",
      "46353.25003950371\n",
      "46173.509259181425\n",
      "46016.84470881993\n",
      "45880.05346568474\n",
      "45760.40126979281\n",
      "45655.55171029992\n",
      "45563.50630862859\n",
      "45482.55380473861\n",
      "45411.22721759577\n",
      "45348.267474033906\n",
      "45292.592588421016\n",
      "45243.27153427436\n",
      "45199.50208283207\n",
      "45160.59199653643\n",
      "45125.94306063188\n",
      "45095.037516450844\n",
      "45067.42652776834\n",
      "45042.72036881125\n",
      "45020.580070794866\n",
      "45000.71030459844\n",
      "44982.85331158598\n",
      "44966.78372361215\n",
      "44952.30413775966\n",
      "44939.24133205464\n",
      "44927.443025889705\n",
      "44916.775103640124\n",
      "44907.11923244605\n",
      "44898.37081566209\n",
      "44890.437232397264\n",
      "44883.23632109057\n",
      "44876.69507144728\n",
      "44870.748494440486\n",
      "44865.338644650365\n",
      "44860.41377306414\n",
      "44855.92759174049\n",
      "44851.83863449215\n",
      "44848.10970010549\n",
      "44844.7073665926\n",
      "44841.60156666086\n",
      "44838.76521601976\n",
      "44836.17388735825\n",
      "44833.80552385788\n",
      "44831.64018699089\n",
      "44829.659834088874\n",
      "44827.84812181902\n",
      "44826.19023223145\n",
      "44824.672718514616\n",
      "44823.28336798892\n",
      "44822.01108019677\n",
      "44820.84575825163\n",
      "44819.7782118392\n",
      "44818.800070489095\n",
      "44817.903705906334\n",
      "44817.08216231615\n",
      "44816.32909390568\n",
      "44815.638708559854\n",
      "44815.00571719954\n",
      "44814.42528809648\n",
      "44813.89300563886\n",
      "44813.404833060646\n",
      "44812.957078728214\n",
      "44812.5463656058\n",
      "44812.16960357786\n",
      "44811.82396433779\n",
      "44811.50685858642\n",
      "44811.21591530789\n",
      "44810.94896292679\n",
      "44810.70401215581\n",
      "44810.479240377\n",
      "44810.27297741248\n",
      "44810.08369254569\n",
      "44809.90998268832\n",
      "44809.75056157601\n",
      "44809.604249906515\n",
      "44809.46996632994\n",
      "44809.34671921639\n",
      "44809.23359912969\n",
      "44809.12977194548\n",
      "44809.034472557905\n",
      "44808.946999117055\n",
      "44808.86670776052\n",
      "44808.79300778596\n",
      "44808.725357232746\n",
      "44808.66325883537\n",
      "44808.606256316765\n",
      "44808.55393099137\n",
      "44808.50589865424\n",
      "44808.4618067277\n",
      "44808.42133164829\n",
      "44808.38417646867\n",
      "44808.35006866329\n",
      "44808.31875811034\n",
      "44808.29001524743\n",
      "44808.26362937452\n",
      "44808.239407103545\n",
      "44808.2171709291\n",
      "44808.19675792477\n",
      "44808.17801854011\n",
      "44808.16081550364\n",
      "44808.14502281119\n",
      "44808.13052479987\n",
      "44808.117215299266\n",
      "44808.104996850045\n",
      "44808.09377998899\n",
      "44808.08348259331\n",
      "44808.07402927543\n",
      "44808.06535083203\n",
      "44808.05738373645\n",
      "44808.05006967165\n",
      "44808.043355101894\n",
      "44808.037190883435\n",
      "44808.03153190075\n",
      "44808.026336738934\n",
      "44808.02156737773\n",
      "44808.01718891462\n",
      "44808.01316930839\n",
      "44808.009479144566\n",
      "44808.00609142042\n",
      "44808.002981346726\n",
      "44808.00012616614\n",
      "44807.997504987434\n",
      "44807.995098630774\n",
      "44807.99288948973\n",
      "44807.9908614006\n",
      "44807.98899952395\n",
      "44807.9872902375\n",
      "44807.98572103534\n",
      "44807.98428043636\n",
      "44807.982957900815\n",
      "44807.98174375233\n",
      "44807.980629108\n",
      "44807.979605812805\n",
      "44807.97866638057\n",
      "44807.97780393812\n",
      "44807.97701217518\n",
      "44807.976285299905\n",
      "44807.97561799435\n",
      "44807.975005376444\n",
      "44807.974442963925\n",
      "44807.97392664278\n",
      "44807.97345263565\n",
      "44807.9730174746\n",
      "44807.97261797608\n",
      "44807.9722512171\n",
      "44807.971914515285\n",
      "44807.97160540688\n",
      "44807.971321630466\n",
      "44807.971061110366\n",
      "44807.97082194015\n",
      "44807.97060237033\n",
      "44807.97040079526\n",
      "44807.970215739275\n",
      "44807.97004584902\n",
      "44807.969889881584\n",
      "44807.969746696035\n",
      "44807.96961524486\n",
      "44807.96949456626\n",
      "44807.96938377771\n",
      "44807.96928206828\n",
      "44807.969188694144\n",
      "44807.96910297215\n",
      "44807.96902427537\n",
      "44807.968952027935\n",
      "44807.96888570117\n",
      "44807.96882480995\n",
      "44807.96876890914\n",
      "44807.96871758921\n",
      "44807.96867047514\n",
      "44807.96862722216\n",
      "44807.96858751357\n",
      "44807.968551059515\n",
      "44807.96851759286\n",
      "44807.968486868755\n",
      "44807.968458662515\n",
      "44807.96843276802\n",
      "44807.96840899552\n",
      "44807.96838717117\n",
      "44807.968367135385\n",
      "44807.96834874142\n",
      "44807.96833185495\n",
      "44807.96831635265\n",
      "44807.96830212035\n",
      "44807.968289054654\n",
      "44807.968277059685\n",
      "44807.96826604768\n",
      "44807.96825593813\n",
      "44807.968246657074\n",
      "44807.96823813679\n",
      "44807.968230314546\n",
      "44807.96822313349\n",
      "44807.968216540896\n",
      "44807.968210488405\n",
      "44807.96820493205\n",
      "44807.968199830975\n",
      "44807.968195148125\n",
      "44807.968190848725\n",
      "44807.968186901926\n",
      "44807.96818327857\n",
      "44807.96817995212\n",
      "44807.96817689798\n",
      "44807.968174094596\n",
      "44807.96817152085\n",
      "44807.968169157764\n",
      "44807.96816698863\n",
      "44807.96816499732\n",
      "44807.96816316871\n",
      "44807.96816149019\n",
      "44807.968159949334\n",
      "44807.96815853503\n",
      "44807.96815723617\n",
      "44807.968156044175\n",
      "44807.968154949325\n",
      "44807.968153944625\n",
      "44807.96815302178\n",
      "44807.96815217499\n",
      "44807.96815139753\n",
      "44807.9681506837\n",
      "44807.96815002855\n",
      "44807.96814942703\n",
      "44807.96814887461\n",
      "44807.96814836778\n",
      "44807.968147902175\n",
      "44807.9681474748\n",
      "44807.968147082414\n",
      "44807.9681467226\n",
      "44807.96814639195\n",
      "44807.96814608835\n",
      "44807.968145809355\n",
      "44807.96814555363\n",
      "44807.96814531878\n",
      "44807.96814510317\n",
      "44807.96814490528\n",
      "44807.968144723425\n",
      "44807.96814455654\n",
      "44807.968144403574\n",
      "44807.96814426285\n",
      "44807.96814413367\n",
      "44807.968144015445\n",
      "44807.96814390646\n",
      "44807.96814380655\n",
      "44807.96814371482\n",
      "44807.968143630795\n",
      "44807.968143553524\n",
      "44807.968143482474\n",
      "44807.96814341727\n",
      "44807.96814335772\n",
      "44807.96814330291\n",
      "44807.968143252496\n",
      "44807.96814320618\n",
      "44807.96814316346\n",
      "44807.96814312448\n",
      "44807.96814308866\n",
      "44807.96814305598\n",
      "44807.9681430256\n",
      "44807.968142998005\n",
      "44807.96814297244\n",
      "44807.968142949154\n",
      "44807.968142927704\n",
      "44807.968142908125\n",
      "44807.968142889935\n",
      "44807.96814287347\n",
      "44807.968142858124\n",
      "44807.968142844205\n",
      "44807.968142831334\n",
      "44807.96814281963\n",
      "44807.968142808815\n",
      "44807.968142798905\n",
      "44807.96814278976\n",
      "44807.968142781516\n",
      "44807.968142773854\n",
      "44807.96814276683\n",
      "44807.96814276026\n",
      "44807.96814275436\n",
      "44807.968142748985\n",
      "44807.96814274388\n",
      "44807.96814273928\n",
      "44807.96814273507\n",
      "44807.968142731144\n",
      "44807.96814272773\n",
      "44807.96814272437\n",
      "44807.96814272144\n",
      "44807.96814271873\n",
      "44807.96814271622\n",
      "44807.96814271373\n",
      "44807.96814271162\n",
      "44807.96814270969\n",
      "44807.96814270801\n",
      "44807.968142706326\n",
      "44807.968142704754\n",
      "44807.968142703336\n",
      "44807.96814270209\n",
      "44807.96814270088\n",
      "44807.9681426999\n",
      "44807.96814269877\n",
      "44807.96814269788\n",
      "44807.96814269714\n",
      "44807.96814269636\n",
      "44807.968142695696\n",
      "44807.96814269502\n",
      "44807.96814269444\n",
      "44807.96814269387\n",
      "44807.968142693426\n",
      "44807.968142692946\n",
      "44807.968142692414\n",
      "44807.968142692116\n",
      "44807.96814269179\n",
      "44807.96814269145\n",
      "44807.9681426911\n",
      "44807.96814269085\n",
      "44807.9681426906\n",
      "44807.968142690246\n",
      "44807.968142690166\n",
      "44807.968142689904\n",
      "44807.968142689715\n",
      "44807.96814268958\n",
      "44807.968142689366\n",
      "44807.96814268924\n",
      "44807.96814268919\n",
      "44807.96814268894\n",
      "44807.96814268889\n",
      "44807.96814268879\n",
      "44807.968142688645\n",
      "44807.96814268858\n",
      "44807.96814268859\n",
      "44807.96814268842\n",
      "44807.968142688434\n",
      "44807.968142688376\n",
      "44807.96814268827\n",
      "44807.968142688245\n",
      "44807.96814268827\n",
      "44807.96814268822\n",
      "44807.96814268815\n",
      "44807.96814268826\n",
      "44807.96814268821\n",
      "44807.96814268819\n",
      "44807.96814268815\n",
      "44807.968142688136\n",
      "44807.968142688085\n",
      "44807.96814268807\n",
      "44807.96814268809\n",
      "44807.96814268805\n",
      "44807.968142688085\n",
      "44807.96814268804\n",
      "44807.96814268803\n",
      "44807.968142688056\n",
      "44807.96814268804\n",
      "44807.96814268808\n",
      "44807.96814268802\n",
      "44807.96814268802\n",
      "44807.96814268801\n",
      "44807.96814268797\n",
      "44807.96814268794\n",
      "44807.96814268794\n",
      "44807.968142687925\n",
      "44807.96814268802\n",
      "44807.96814268799\n",
      "44807.96814268796\n",
      "44807.96814268798\n",
      "44807.96814268799\n",
      "44807.968142687954\n",
      "44807.96814268796\n",
      "44807.96814268799\n",
      "44807.96814268797\n",
      "44807.968142687976\n",
      "44807.96814268798\n",
      "44807.96814268798\n",
      "44807.96814268794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44807.96814268797\n",
      "44807.968142687976\n",
      "44807.968142687976\n",
      "44807.968142688\n",
      "44807.96814268797\n",
      "44807.96814268793\n",
      "44807.96814268792\n",
      "44807.96814268791\n",
      "44807.96814268791\n",
      "44807.96814268795\n",
      "44807.96814268794\n",
      "44807.96814268794\n",
      "44807.96814268797\n",
      "44807.96814268795\n",
      "44807.96814268793\n",
      "44807.96814268792\n",
      "44807.968142687874\n",
      "44807.96814268788\n",
      "44807.96814268786\n",
      "44807.968142687845\n",
      "44807.96814268786\n",
      "44807.968142687874\n",
      "44807.96814268787\n",
      "44807.968142687874\n",
      "44807.968142687874\n",
      "44807.96814268789\n",
      "44807.96814268791\n",
      "44807.96814268791\n",
      "44807.9681426879\n",
      "44807.968142687896\n",
      "44807.96814268791\n",
      "44807.96814268788\n",
      "44807.96814268789\n",
      "44807.96814268787\n",
      "44807.968142687896\n",
      "44807.968142687896\n",
      "44807.96814268789\n",
      "44807.96814268788\n",
      "44807.9681426879\n",
      "44807.96814268787\n",
      "44807.968142687874\n",
      "44807.968142687874\n",
      "44807.96814268784\n",
      "44807.96814268784\n",
      "44807.96814268784\n",
      "44807.968142687874\n",
      "44807.96814268786\n",
      "44807.96814268785\n",
      "44807.96814268785\n",
      "44807.96814268785\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def df(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "nn = NeuralNet([50, 50, 2], [f, f, f], [df, df, df], [[20, 10 ,40], [30, 15, 15], [15, 25, 24]], [[800,200], [856, 400], [900, 444]], 0.000000000005)\n",
    "\n",
    "print(nn.getLoss())\n",
    "for i in range(500):\n",
    "    nn.getGradient()\n",
    "    print(nn.getLoss())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(list(range(2 - 2, -1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = [4, 5, 6]\n",
    "np.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58049094, 0.54493143, 0.25574428],\n",
       "       [0.33914321, 0.8644203 , 0.39340868],\n",
       "       [0.79962713, 0.47052293, 0.0560082 ],\n",
       "       [0.1487162 , 0.66520349, 0.89362601]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(3, 4, f, df)\n",
    "layer.getWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58049094, 0.54493143, 0.25574428],\n",
       "       [0.33914321, 0.8644203 , 0.39340868],\n",
       "       [0.79962713, 0.47052293, 0.0560082 ],\n",
       "       [0.1487162 , 0.66520349, 0.89362601]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.getWeights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a', '1'), (2, 'b', '2'), (3, 'c', '3')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "c = ['1', '2', '3']\n",
    "\n",
    "list(zip(a, b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
